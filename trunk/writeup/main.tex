%\usepackage[margin=1in]{geometry}
 
\usepackage{times,graphicx,color}
\usepackage{amsmath, amssymb, stmaryrd}
\usepackage{ttquot,utf8}
\usepackage[comments]{declarations}

\renewcommand{\floatpagefraction}{0.75}
\renewcommand{\dblfloatpagefraction}{0.75}

\title{General Error Diagnostic for Program Analyses}

\ifanonymous
  \authorinfo{Anonymous}{}{}
\else
\authorinfo{Danfeng Zhang}
           {Department of Computer Science\\ Cornell University\\ Ithaca, NY, 14853}
           {zhangdf@cs.cornell.edu}
\authorinfo{Andrew C. Myers}
           {Department of Computer Science\\ Cornell University\\ Ithaca, NY, 14853}
           {andru@cs.cornell.edu}
\fi
\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Nowadays, more and more type systems relies on constraint solving. For
instance, Jif ... Type checking for ML-like language is also by nature
constraint solving. However, it has also been acknowledged for
decades that error diagnostic for such systems are difficult
since the type-checker often reports error locations that are far from the
simplest source of the problem~\cite{wand-errorfinding}.

%\paragraph{Contributions}

This paper makes the following contributions:

\begin{enumerate}
\item
general representation

\item
application to error diagnosis

\item
evaluation
\end{enumerate}

\section{Program analyses and constraint solving}

Many program analyses can be modeled as constraint solving problems. In this
section, we use two apparently different kinds of analyses, information-flow
control and ML-like language type inference, to motivate why error diagnostic
is difficult and illustrate the main approach of our work.


\section{Constraint language}
\label{sec:language}

In this section, we describe the core constraint language capturing the essence
of various program analyses. How information flow control and ML-like type
inference can be expressed by this language is also discussed. 

\subsection{Syntax}

The syntax of our general constraint language is formalized in
Figure~\ref{figure:lang:syntax}.

\begin{figure}
\begin{align*}
G &::=\; G_1 \land G_2\ |\ A \\
A &::=\; C_1 \proves C_2 \\
C &::= C_1 \land C_2\ |\  E_1 \leq E_2 |\ \Empty \\
E &::= x\ |\ c(E_1,\dots,E_{a(c)})\ |\ c^{-i}(E)\ |\ E_1 \join E_2 \
|\ E_1 \meet E_2
\end{align*}
\caption{Syntax of constraints}
\label{figure:lang:syntax}
\end{figure}

The top-level goal $G$ to be solved is a conjunction of assertions $A$. An
assertion has the form form $C_1 \proves C_1$, where constraint $C_1$ is the
hypothesis whereas constraint $C_2$ is the conclusion needs to be satisfied. We
write $ \proves C_2$ instead of $ C_1 \proves C_2$ when $C_1=\Empty$.

Constraint $C$, either serving as the hypothesis or the conclusion, is a
conjunction of partial orders on the elements $E$.

Let $\varset$ be a set of variables and $\conset$ be a set of constructors.
Each $c\in C$ has a fixed arity $a(c)$. When $a(c)=0$ then $c$ is a constant.

Element $E$ contains the variables to be inferred $x\in V$, constructors $c$
and corresponding projection $c^{-i}$, where $c\in C$. For simplicity, we
write $c$ for constants. Join ($\join$) and meet ($\meet$) of elements models
the least upper bound/greatest lower bound of two elements in the partial order
respectively. When join and meet are used, we assume for all $e_1, e_2 \in E,
e_1 \join e_2 \in E \land e_1 \meet e_2 \in E$ to make the constraints
well-formed. In another word, the elements form a lattice.

We say an assertion $A = C_1 \proves C_2$ is _valid_ if the all the partial
orders in $C_2$ can be inferred under assumption $C_1$, assuming that $\leq$ is
a partial order and the definition of $\join$ and $\meet$.

\paragraph{Example}

Let $c_1, c_2, c_3$ be three constants. Then $c_1 \leq c_2 \land c_2 \leq c_3
\proves c_1 \leq c_3$ is valid by the transitivity of $\leq$. Assertion $
\proves c_1 \leq c_1 \join c_2$ is valid by the definition of join. 

\subsection{Satisfiability}

Intuitively, a goal $G$ is _satisfiable_ if there exists a valuation of
variables such that all partial orders in $C_2$ can be satisfied under
hypothesis $C_1$.

More formally, let $\conset$ be all _ground terms_, which forms a Herbrand
universe. A valuation $\valuation: v \to g, v\in \varset, g\in \groundset$ is a
function from variables to ground terms. A goal is satisfiable when there
exists a valuation s.t. all constraints are _valid_. 

\paragraph{Example}

Let $x\in \varset$, $c_1, c_2, c_3\in \groundset$. Then $\proves x \leq c_1$ is
trivially satisfiable considering the valuation $\valuation(x)=c_1$. However,
$\proves x \leq c_1 \land c_2 \leq x$ is unsatisfiable since otherwise, $c_2
\leq c_1$ by the transitivity of $\leq$. But such partial order on $c_1, c_2$
is nowhere to be inferred.

\subsection{Error diagnostic}

Based on the definitions above, the goal of this paper is to answer which
element or constraint is most likely be wrong when a goal is unsatisfiable. 

In general, there are two categories of causes: missing hypothesis and
inconsistent constraints. For instance, consider the unsatisfiable goal
$\proves x \leq c_1 \land c_2 \leq x$.

The possible reasons may be the missing of an hypothesis $c_1\leq c_2$, or one
of the constraints $\proves x \leq c_1$ and $c_2 \leq x$. We discuss general
diagnostic approach for both kinds of causes in Section~\ref{sec:ranking}.

For type inference:

Type inference is reduced to constraint solving by defining a mapping
of _pre-judgements_to constraints:
\[\trans{\G \proves x:\tau} = x =\tau\]
\[\trans{\G \proves \lambda x.e:\tau} = \exists a_1 a_2.
(def x: a_1 in \trans{e:a_2} \land a_1 \rightarrow a_2 =\tau)\]
\[\trans{\G\proves e_1 e_2 : \tau} = \exists a.(\trans{\G \proves
e_1:a\rightarrow \tau}\land \trans{\G\proves e_2:a})\]

\DZ{We may eliminate the $\exists$ by creating fresh variables}

\paragraph{Relation with set constraints}

Although the core constraint language we proposed is similar to set
constraints~\cite{aiken-setconstraint} in spirit. There are several
noticeable differences:

\begin{enumerate}

\item Most importantly, hypotheses is modeled in our language. This extended
expressiveness is essential for modeling real-world program analyses, such as
the ``where'' clause in Jif and the subtyping relationship in many
object-oriented systems. Moreover, as we will show in ..., introducing such
expressiveness does not increase the symbolic complicity of our error
diagnostic algorithm.

\item The exclusion of the negation of an element in our model since
it is not essential in modeling the type systems we are aware of.
Also, previous work showed that although solving the full set of set
constraints is decidable, the complexity ranges from NP to NEXPTIME
for non-trivial ground term models~\cite{aiken-complexity}. We believe
that the subset we used in this work is both expressive and practical
for real-world type systems.

\end{enumerate}

\subsection{Expressiveness}

Due to the similarity between our core constraint language and set
constraint, the applications modeled by the later without negations
are automatically expressed by our model. Such examples includes type
inference with sum type and product type and closure analysis.

In this part, we discuss how a information-flow control model DLM, and ML-like
type inference with polymorphism can be expressed by our constraint language.

\paragraph{DLM model}

For information flow control, elements are security labels, which in
nature forms a lattice~\cite{denning-lattice}. Join and meet
operations in constraints are consistent with that in the lattice.
$\leq$ is consistent with the partial order in lattice.
 
In this part, we focus on the DLM model, although other models would be simpler
to be modeled. The DLM model is complicated in the following ways:

\begin{enumerate}
\item The decentralized model brings more complexity in the labels.

\item DLM introduces an implicit form of parametric polymorphism, called _label
polymorphism_, to express procedures that are parametric with respect to the
security labels of their arguments.

\item Jif allows ``where'' clauses. The ``where'' clauses makes static
assumptions on the partial orders on principals or labels. The security of a
program is checked based on such assumptions, while the validity of the
assumptions are checked at runtime. 
\end{enumerate}

DLM assumes a set of _principals_ represienting users and other
authority entities. Examples of principals could be student A,
students, teachers and so on. The security concern of information is
expressed by _labels_. A label $L$ contains a set of principals called
the _owners_, $owners(L)$. Owners are the original sources of the
information. For each owner $O$, the label also contains a set of
principals called the _readers_, $readers(L,O)$. Readers are the
principals owner $O$ is willing to release the value. A useful concept
is _effective reader set_ of $L$: the set of principal that is allowed
to read by all owners.

For instance, for label $\{o_1:\ r_1,r_2;\ o_2:\ r_2,r_3\}$, the
effective reader set is just $\{r_2\}$. In a high level, such labels
abstracts allowed flows of the information. For instance, the label
above only allows data from $o_1, o_2$ to $r_2$, but not restricting
the data of other owners.

Such allowed flows are complicated by the _actsfor_ relationship of
principals. For instance, if we know that $r_4$ actsfor $r_2$, then
$r_4$ should also be able to read the data above.

\paragraph{ML-like type inference}

Type inference can also naturally map into constraint solving. This
view is not new. For instance, Wand~\cite{wand-typeinference} recast
Hindley-Milner type system into equality constraints. Aiken and
Wimmers~\cite{aiken-typeinclusion} extends the Hindley-Milner type
system with inclusion constraints, and models function types,
constructor types, liberal intersection and union types. In this setting, types
are the elements. Join and meet operations are the usual intersection and union
types if that is well defined.  $\leq$ is consistent with the subtype
relationship between two labels.  Since the definition is abstract, both
syntactic subtyping \DZ{REF} and semantic subtyping~\cite{aiken-typeinclusion}
fit in this model.

\paragraph{Polymorphic Types}

Our model is also powerful enough to capture polymorphic types.  Jif
has polymorphic types, for instance, $caller\_pc$. Polymorphic types
are also crucial for ML-like languages.

Polymorphic types can be naturally modeled in our core constraint
language as follows. For a polymorphic type $t$ instance, we create a
new constant $c_t$ and replace all occurrences of $t$ with $c_t$ in
the constraints. This is sufficient since the semantics of a
polymorphic type is that the constraints should be satisfied
regardless of the value of $t$. For instance, $t\leq \top$ where $t$
is a polymorphic type is satisfiable.

\section{Graph} 

In this section, we transform the constraint-solving problem into the
reachability in a graph: to find a path in graph where no partial order on the
source and sink exist. 

Transformation ... 
%The insight is consistent with ``A flow model FM is _secure_ if and only if
%execution of a sequence of operations cannot give rise to a flow that violates
%the relation $\rightarrow$''~\cite{denning-lattice}.

% First, for all the constraints in the right-hand-side, we
% first split the constraints into _atomic_ form according to the
% straightforward rules \DZ{only when the lattice is distributive}:
% 
% \[
% \G \proves l_1 \meet l_2 \meet \dots \meet l_m \leq r_1 \join r_2 \join \dots
% \join r_n
% \]
% 
% For constraints in this form, we generate nodes for each side, and
% add one _conditional edge_.
% 
% Static edges?
% 
% Back edges?
% 
% \DZ{No need} Conservative edge. When the right part constrains more
% than one variable, in general, satisfiability would be undecidable. As
% a common practice\DZ{true?}, we conservatively add edges from the
% union node to all variable components.
% 
\section{Ranking}
\label{sec:ranking}

\subsection{Inferring missing assumptions}

\subsection{Inferring likely wrong entities in program}

MAP model

\section{Evaluation}

\subsection{Extensibility}

\subsection{Ranking quality}

\section{Related Work}

Modeling program analyses via constraint solving is not a new idea. The most
related work is set-based constraint program
analysis~\cite{aiken-setconstraint, aiken-typeinclusion}.  However, such
constraints does not model hypothesis, which is important in program analyses
such as information-flow control.
 
Program slicing, shape analysis, flow-insensitive points-to analysis are
amenable to graph-reachability~\cite{reps-graph}. Melski and
Reps~\cite{melski-cflgraph} noticed the equality between context-free-language
reachability (CFG-reachability) and a subset of set-based constraints. But only
a small set of constraints, namely only one variable may appear on the right
hand side of an partial order. Moreover, no general error diagnostic approach
is proposed on the graphs to the best of our knowledge. 

Due to the unsatisfactory quality of error reports, a considerable amount of
work have been proposed to improve both error message of ML-like languages and
Jif.

Efforts on improving ML-like language type-error message can be traced back to
the early work of Wand~\cite{wand-errorfinding} and Johnson and
Walz~\cite{johnson-popl86}. These two pieces of work represent two directions
in improving type-error messages: the former one traces _everything_ that
contributes to the error, whereas the latter attempts to infer the _most
likely_ cause. We only discuss the most related among them in this paper. More
details can be found in Heeren's summary~\cite{heeren:thesis}.

In the first direction, many work~\cite{choppella95, haack:slicing,
tip:slicing} follow improves the basic idea of Wand~\cite{wand-errorfinding} in
several ways. Despite the attractiveness of feeding all related details to the
programmer, the reports are usually verbose and hard to follow.

In the second direction, one approach is to alter the order of type
unification~\cite{lee:toplas, mcadam:unification}. However, since error
location may appear anywhere in the unification procedure, any specific order
would fail. Some previous work try to advice the programmer how to fix the
error~\cite{mcadam:thesis, lerner:pldi07}. McAdam~\cite{mcadam:thesis} suggests
using morphisms along with standard type inference to identify potential fixes
of type errors. Lerner et. al.~\cite{lerner:pldi07} try to produce type-error
messages by searching changes to the AST until a program type checks. However,
both approaches are highly specific to the purpose of type inference. There is
no easy way of extending them for information-flow control analysis, for
instance.

For information-flow control, King et. al.~\cite{king:fse} propose to
generate a trace of information flow explaining the information-flow violation
in program. Despite the fact that this approach also diagnoses on a constructed
dependency graph, only a subset of DLM model is handled (hypothesis is not
supported). Moreover, similar to the limitation of type-error slicing,
reporting a whole path can be verbose for programmer.
Merlin~\cite{livshits:merlin} uses probabilistic inference to automatically
infer explicit information flow specifications from program code, an different
application as our paper.

The most related work on inferring likely missing hypothesis is the recent work
on error diagnosis using abductive inference~\cite{dillig:pldi12}. This work
computes small, relevant queries presented to a user that capture exactly the
information a program analysis is missing to either discharge or validate the
error. But it is handling first-order logic and there is no straightforward way
of encoding our constraint language into first-order logic. Also, our
missing-hypothesis inference is more straightforward.

% Automatic instrumentation~\cite{king:esop10}
% Policy inference~\cite{chong:sp11, harris:ccs10}.
% Using statisctical
% analysis for bug finding is not new. Dawson Engler.

\bibliographystyle{abbrv}
\bibliography{constraint,../bibtex/pm-master}

\end{document}
